{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 5 - Visit the Wikipedia hyperlinks graph!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import networkx as nx\n",
    "import pickle\n",
    "import collections\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import statistics\n",
    "import operator\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Researh question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[RQ1] Build the graph [G=(V, E)] , where V is the set of articles and E the hyperlinks among them, and provide its basic information:\n",
    "\n",
    "* If it is direct or not\n",
    "* The number of nodes\n",
    "* The number of edges\n",
    "* The average node degree. Is the graph dense?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading reduced edges file as dataframe, where one column is the source and other is the destination\n",
    "source_destination=pd.read_csv('wiki-topcats-reduced.txt',sep=\"\\t\",header=None,names=[\"source\",\"destination\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">reading of a dicionary created in _Homework 5 - RQ1 pre-check up.ipynb_ notebook file\n",
    "source_destination_dict is a **default dictionary** with list as datatype for value of dictionary, where **key**=source_id(article_id), **value**=destination_id(list of article_id's which are connected to that article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#reading pickle file into a memory, which contains a dictionary where **key=source_id(article_id)**, **value=destination_id(list of article_id's** which are connected to that article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_destination_dict = pickle.load(open('source_destination_dict.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2645247it [03:07, 14115.72it/s]\n"
     ]
    }
   ],
   "source": [
    "#list of (source, destination) tuples which will be used to read edges from them in networkx _add_edges_from_ method\n",
    "source_destination_tuples=[(row[\"source\"],row[\"destination\"]) for idx,row in tqdm(source_destination.iterrows())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The number of nodes and edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the results and the conclusion from the _Homework 5 - RQ1 pre-check up.ipynb_ we can use DiGraph() method to make a directed graph immediately. And check again the subquestions of the RQ1 in order to give the final response and conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 461193 \n",
      "Number of edges: 2645247\n"
     ]
    }
   ],
   "source": [
    "Gtup_directed = nx.DiGraph()\n",
    "Gtup_directed.add_edges_from(source_destination_tuples)\n",
    "print(\"Number of nodes:\",len(Gtup_directed.nodes()),\"\\nNumber of edges:\",len(Gtup_directed.edges()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average node degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: \n",
      "Type: DiGraph\n",
      "Number of nodes: 461193\n",
      "Number of edges: 2645247\n",
      "Average in degree:   5.7357\n",
      "Average out degree:   5.7357\n"
     ]
    }
   ],
   "source": [
    "print(nx.info(Gtup_directed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Is the graph dense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Density of the graph: 1.2436602635647606e-05\n"
     ]
    }
   ],
   "source": [
    "print('Density of the graph:',nx.density(Gtup_directed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The graph is a sparse graph. Because the density has a small value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results of the RQ1:\n",
    "    \n",
    ">The graph is **DIRECTED**. That is why we used DiGraph() method from nx library\n",
    " \n",
    ">The **number of nodes**: 461 193\n",
    "\n",
    ">The **number of edges**: 2 645 247\n",
    "\n",
    ">The **average node degree** is: 5.7357\n",
    "\n",
    ">Is the graph dense? Density = 1.2437e-05 The density can have value from 0 to 1, whereby it is 0 for a graph without edges and 1 for a complete graph. Therefore we can conclude that **the graph is sparse** and not dense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Researh question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Building Block Ranking\n",
    "\n",
    "    > Based on the implementation of the **shortest path** algorithm compare sample number of nodes of C0-input category with all nodes in all the other Ci categories in order to build the **block ranking**. \n",
    "\n",
    "2. Ranking nodes of each category in the block ranking vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#categories file, where each category has a list of articles connected to it\n",
    "categories=pd.read_csv('wiki-topcats-categories.txt',sep=\"\\n\",header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_cat(row):\n",
    "   \n",
    "    \"\"\"\n",
    "    method which performs cleaning of each row from the categories file and counting the actual number of articles that belong to that category\n",
    "  \n",
    "    returns the number of articles \n",
    "    \"\"\"\n",
    "    return len(row.split(\"; \")[1].split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying the lambda and num_cat method for labeling and selecting categories who have more than 3500 articles\n",
    "categories=categories[categories.iloc[:,0].apply(lambda x:True if num_cat(x)>3500 else False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning the dataframe with categories and mapping the articles to integers.\n",
    "#and making the categories_dict where key=category name, value=list of article id's belonging to that category\n",
    "categories_dict=defaultdict(list)\n",
    "for idx in categories.index:\n",
    "    cat_and_values=categories[0].loc[idx].split(\"; \")\n",
    "    cat_name=cat_and_values[0].split(\":\")[1]\n",
    "    categories_dict[cat_name]=list(map(int,cat_and_values[1].split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We should consider as nodes, only the ones given in the reduced version of the graph. Therefore we are checking and removing\n",
    "#the other ones and based on that 6 more categories are eliminated. Therefore, now we have 29 categories.\n",
    "categories = {}\n",
    "all_nodes=set(Gtup_directed.nodes())\n",
    "for key, values in categories_dict.items():\n",
    "        categories[key] = all_nodes.intersection(set(values))\n",
    "        if len(categories[key]) < 3500:\n",
    "            del(categories[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">In order to test and make a runnable version of our code, we decided to test it on the sample. We have chosen the **'Windows_games'** as our **input category** and we will perform random sample, which is ok because it is a really sparse graph, and we decided select **100 nodes** from the input category and **all the other nodes** from the rest of the categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_edges={}\n",
    "for cat in categories:\n",
    "    cat_edges[cat]=len(Gtup_directed.subgraph(categories[cat]).edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'English_footballers': 18233,\n",
       " 'The_Football_League_players': 19672,\n",
       " 'Association_football_forwards': 4710,\n",
       " 'Association_football_goalkeepers': 8209,\n",
       " 'Association_football_midfielders': 3801,\n",
       " 'Association_football_defenders': 2286,\n",
       " 'Living_people': 1218406,\n",
       " 'Harvard_University_alumni': 3959,\n",
       " 'Major_League_Baseball_pitchers': 10472,\n",
       " 'Members_of_the_United_Kingdom_Parliament_for_English_constituencies': 30039,\n",
       " 'Indian_films': 3323,\n",
       " 'Year_of_death_missing': 1242,\n",
       " 'Year_of_birth_missing_(living_people)': 6814,\n",
       " 'Rivers_of_Romania': 15241,\n",
       " 'Main_Belt_asteroids': 10891,\n",
       " 'Asteroids_named_for_people': 142,\n",
       " 'English-language_albums': 7064,\n",
       " 'British_films': 2805,\n",
       " 'English-language_films': 21278,\n",
       " 'American_films': 10372,\n",
       " 'People_from_New_York_City': 3156,\n",
       " 'American_television_actors': 32231,\n",
       " 'American_film_actors': 53164,\n",
       " 'Debut_albums': 779,\n",
       " 'Black-and-white_films': 6288,\n",
       " 'Year_of_birth_missing': 1112,\n",
       " 'Place_of_birth_missing_(living_people)': 736,\n",
       " 'American_military_personnel_of_World_War_II': 5133,\n",
       " 'Windows_games': 1}"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('American_film_actors', 53164),\n",
       " ('American_films', 10372),\n",
       " ('American_military_personnel_of_World_War_II', 5133),\n",
       " ('American_television_actors', 32231),\n",
       " ('Association_football_defenders', 2286),\n",
       " ('Association_football_forwards', 4710),\n",
       " ('Association_football_goalkeepers', 8209),\n",
       " ('Association_football_midfielders', 3801),\n",
       " ('Asteroids_named_for_people', 142),\n",
       " ('Black-and-white_films', 6288),\n",
       " ('British_films', 2805),\n",
       " ('Debut_albums', 779),\n",
       " ('English-language_albums', 7064),\n",
       " ('English-language_films', 21278),\n",
       " ('English_footballers', 18233),\n",
       " ('Harvard_University_alumni', 3959),\n",
       " ('Indian_films', 3323),\n",
       " ('Living_people', 1218406),\n",
       " ('Main_Belt_asteroids', 10891),\n",
       " ('Major_League_Baseball_pitchers', 10472),\n",
       " ('Members_of_the_United_Kingdom_Parliament_for_English_constituencies',\n",
       "  30039),\n",
       " ('People_from_New_York_City', 3156),\n",
       " ('Place_of_birth_missing_(living_people)', 736),\n",
       " ('Rivers_of_Romania', 15241),\n",
       " ('The_Football_League_players', 19672),\n",
       " ('Windows_games', 1),\n",
       " ('Year_of_birth_missing', 1112),\n",
       " ('Year_of_birth_missing_(living_people)', 6814),\n",
       " ('Year_of_death_missing', 1242)]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(cat_edges.items(), key=operator.itemgetter(0), reverse= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random sampling of 100 nodes from the input category 'Windows_games' and making a sample subgraph for it\n",
    "for i in range(100):\n",
    "    cat_nodes_lst = random.sample(categories['American_film_actors'], 100)\n",
    "    Gtup_directed_sample_C0=Gtup_directed.subgraph(cat_nodes_lst)\n",
    "    if len(Gtup_directed_sample_C0.subgraph(categories['American_film_actors']).edges)>10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53164"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Gtup_directed.subgraph(categories['American_film_actors']).edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Name: \\nType: SubDiGraph\\nNumber of nodes: 100\\nNumber of edges: 13\\nAverage in degree:   0.1300\\nAverage out degree:   0.1300'"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nx.info(Gtup_directed_sample_C0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bfs_shortest_path(graph, start):\n",
    "    \"\"\"\n",
    "    method which returns shorthest paths dictionary where key=node, and value is the actual distance of the shortest path from the start node to the other nodes of the graph\n",
    "    \"\"\"\n",
    "    explored = set()\n",
    "    queue = [start]\n",
    "    shortest_path_dict = {}\n",
    "    counter_level = 0\n",
    "    to_visit_list = []\n",
    "    while queue:\n",
    "        for node in queue:\n",
    "            if node not in explored:\n",
    "                shortest_path_dict[node] = counter_level\n",
    "                neighbours = graph[node]\n",
    "                explored.add(node)\n",
    "                to_visit_list.extend(neighbours)\n",
    "        queue = []\n",
    "        queue.extend(to_visit_list)\n",
    "        to_visit_list = []\n",
    "        counter_level += 1\n",
    "    return shortest_path_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [02:36<00:00,  1.62s/it]\n"
     ]
    }
   ],
   "source": [
    "#calculate shorthest path from each article of the Graph input category sample to all the other nodes in the Graph\n",
    "l = []\n",
    "for article in tqdm(Gtup_directed_sample_C0.nodes()):\n",
    "    l.append(bfs_shortest_path(Gtup_directed, article))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we made sample of 100 nodes from the input category now we have to remove other articles in the category **['Windows_games'].**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to do that we performed the following steps:\n",
    "\n",
    "1. Making of a dictionary **d** to store value of shortest paths, where key=node, value=list of shortest paths distances\n",
    "2. Making of a dictionary **z** to store value of **MINIMUM shortest path**, where key=node, value=minimum of the shortest path\n",
    "3. Making of a **categories_edited**, whose category **'Windows_games'** will be consisted only of the **sampled articles**.\n",
    "4. Making of a **cat_inv_dic** where key=name of the category, value=shortest path values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = defaultdict(list)\n",
    "for node in l:\n",
    "    for key, value in node.items():\n",
    "        d[key].append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = defaultdict(list)\n",
    "for key, value in d.items():\n",
    "    z[key] = np.min(d[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_edited = categories.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_delete_article = []\n",
    "for article in categories_edited['American_film_actors']:\n",
    "    if article not in cat_nodes_lst:\n",
    "        to_delete_article.append(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "for article in to_delete_article:\n",
    "    categories_edited['American_film_actors'].remove(article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see now 'Windows_games' has **100 articles** the one that we sampled from the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(categories_edited['American_film_actors'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(categories_edited.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 29/29 [00:02<00:00, 14.19it/s]\n"
     ]
    }
   ],
   "source": [
    "cat_inv_dic=defaultdict(list)\n",
    "for cat_k,cat_v in tqdm(categories_edited.items()):\n",
    "    for node,val in z.items():\n",
    "        if node in cat_v:\n",
    "            cat_inv_dic[cat_k].append(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cat_inv_dic['American_film_actors'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We didn't consider the calculation of the shortest paths when 2 nodes are not connected, where the 'shortest path' would have been infinite. Since infinite doesn't make sense for median calculation we thought we could add a big number, such as 100.\n",
    "So where the edges between 2 nodes don't exist we extend it to more 100 values so it is considered in the median calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat_k in categories_edited.keys():\n",
    "    cat_inv_dic[cat_k].extend([100]*(len(categories_edited[cat_k])-len(cat_inv_dic[cat_k])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in cat_inv_dic.items():\n",
    "    cat_inv_dic[key] = statistics.median(cat_inv_dic[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'English_footballers': 6.0,\n",
       "             'The_Football_League_players': 6.0,\n",
       "             'Association_football_forwards': 8,\n",
       "             'Association_football_goalkeepers': 100,\n",
       "             'Association_football_midfielders': 100,\n",
       "             'Association_football_defenders': 11.0,\n",
       "             'Living_people': 5.0,\n",
       "             'Harvard_University_alumni': 5,\n",
       "             'Major_League_Baseball_pitchers': 5.0,\n",
       "             'Members_of_the_United_Kingdom_Parliament_for_English_constituencies': 5,\n",
       "             'Indian_films': 4.0,\n",
       "             'Year_of_death_missing': 100.0,\n",
       "             'Year_of_birth_missing_(living_people)': 6.0,\n",
       "             'Rivers_of_Romania': 6,\n",
       "             'Main_Belt_asteroids': 100.0,\n",
       "             'Asteroids_named_for_people': 100,\n",
       "             'English-language_albums': 4.0,\n",
       "             'British_films': 3.0,\n",
       "             'English-language_films': 3,\n",
       "             'American_films': 3,\n",
       "             'People_from_New_York_City': 4.0,\n",
       "             'American_television_actors': 3,\n",
       "             'American_film_actors': 0.0,\n",
       "             'Debut_albums': 4,\n",
       "             'Black-and-white_films': 3,\n",
       "             'Year_of_birth_missing': 100.0,\n",
       "             'Place_of_birth_missing_(living_people)': 5.0,\n",
       "             'American_military_personnel_of_World_War_II': 5.0,\n",
       "             'Windows_games': 5.0})"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cat_inv_dic, where key=name of the category, value is the med\n",
    "cat_inv_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_inv_dic_rank= sorted(cat_inv_dic.items(), key=operator.itemgetter(1), reverse= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('American_film_actors', 0.0),\n",
       " ('British_films', 3.0),\n",
       " ('English-language_films', 3),\n",
       " ('American_films', 3),\n",
       " ('American_television_actors', 3),\n",
       " ('Black-and-white_films', 3),\n",
       " ('Indian_films', 4.0),\n",
       " ('English-language_albums', 4.0),\n",
       " ('People_from_New_York_City', 4.0),\n",
       " ('Debut_albums', 4),\n",
       " ('Living_people', 5.0),\n",
       " ('Harvard_University_alumni', 5),\n",
       " ('Major_League_Baseball_pitchers', 5.0),\n",
       " ('Members_of_the_United_Kingdom_Parliament_for_English_constituencies', 5),\n",
       " ('Place_of_birth_missing_(living_people)', 5.0),\n",
       " ('American_military_personnel_of_World_War_II', 5.0),\n",
       " ('Windows_games', 5.0),\n",
       " ('English_footballers', 6.0),\n",
       " ('The_Football_League_players', 6.0),\n",
       " ('Year_of_birth_missing_(living_people)', 6.0),\n",
       " ('Rivers_of_Romania', 6),\n",
       " ('Association_football_forwards', 8),\n",
       " ('Association_football_defenders', 11.0),\n",
       " ('Association_football_goalkeepers', 100),\n",
       " ('Association_football_midfielders', 100),\n",
       " ('Year_of_death_missing', 100.0),\n",
       " ('Main_Belt_asteroids', 100.0),\n",
       " ('Asteroids_named_for_people', 100),\n",
       " ('Year_of_birth_missing', 100.0)]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Block ranking vector\n",
    "cat_inv_dic_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Category name', 'Median']\n",
    "\n",
    "block_ranking_df = pd.DataFrame([x for x in cat_inv_dic_rank], columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_ranking_df.index.names = ['Ranking']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category name</th>\n",
       "      <th>Median</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ranking</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>American_film_actors</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>British_films</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>English-language_films</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>American_films</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>American_television_actors</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Black-and-white_films</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Indian_films</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>English-language_albums</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>People_from_New_York_City</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Debut_albums</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Living_people</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Harvard_University_alumni</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Major_League_Baseball_pitchers</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Members_of_the_United_Kingdom_Parliament_for_E...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Place_of_birth_missing_(living_people)</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>American_military_personnel_of_World_War_II</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Windows_games</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>English_footballers</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>The_Football_League_players</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Year_of_birth_missing_(living_people)</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Rivers_of_Romania</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Association_football_forwards</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Association_football_defenders</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Association_football_goalkeepers</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Association_football_midfielders</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Year_of_death_missing</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Main_Belt_asteroids</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Asteroids_named_for_people</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Year_of_birth_missing</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Category name  Median\n",
       "Ranking                                                           \n",
       "0                                     American_film_actors     0.0\n",
       "1                                            British_films     3.0\n",
       "2                                   English-language_films     3.0\n",
       "3                                           American_films     3.0\n",
       "4                               American_television_actors     3.0\n",
       "5                                    Black-and-white_films     3.0\n",
       "6                                             Indian_films     4.0\n",
       "7                                  English-language_albums     4.0\n",
       "8                                People_from_New_York_City     4.0\n",
       "9                                             Debut_albums     4.0\n",
       "10                                           Living_people     5.0\n",
       "11                               Harvard_University_alumni     5.0\n",
       "12                          Major_League_Baseball_pitchers     5.0\n",
       "13       Members_of_the_United_Kingdom_Parliament_for_E...     5.0\n",
       "14                  Place_of_birth_missing_(living_people)     5.0\n",
       "15             American_military_personnel_of_World_War_II     5.0\n",
       "16                                           Windows_games     5.0\n",
       "17                                     English_footballers     6.0\n",
       "18                             The_Football_League_players     6.0\n",
       "19                   Year_of_birth_missing_(living_people)     6.0\n",
       "20                                       Rivers_of_Romania     6.0\n",
       "21                           Association_football_forwards     8.0\n",
       "22                          Association_football_defenders    11.0\n",
       "23                        Association_football_goalkeepers   100.0\n",
       "24                        Association_football_midfielders   100.0\n",
       "25                                   Year_of_death_missing   100.0\n",
       "26                                     Main_Belt_asteroids   100.0\n",
       "27                              Asteroids_named_for_people   100.0\n",
       "28                                   Year_of_birth_missing   100.0"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_ranking_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see 'Windows_games' is our input category and it has median 0 which is logical cause it is an input category and shortest distances are 0 between that category and its nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['American_film_actors',\n",
       " 'British_films',\n",
       " 'English-language_films',\n",
       " 'American_films',\n",
       " 'American_television_actors',\n",
       " 'Black-and-white_films',\n",
       " 'Indian_films',\n",
       " 'English-language_albums',\n",
       " 'People_from_New_York_City',\n",
       " 'Debut_albums',\n",
       " 'Living_people',\n",
       " 'Harvard_University_alumni',\n",
       " 'Major_League_Baseball_pitchers',\n",
       " 'Members_of_the_United_Kingdom_Parliament_for_English_constituencies',\n",
       " 'Place_of_birth_missing_(living_people)',\n",
       " 'American_military_personnel_of_World_War_II',\n",
       " 'Windows_games',\n",
       " 'English_footballers',\n",
       " 'The_Football_League_players',\n",
       " 'Year_of_birth_missing_(living_people)',\n",
       " 'Rivers_of_Romania',\n",
       " 'Association_football_forwards',\n",
       " 'Association_football_defenders',\n",
       " 'Association_football_goalkeepers',\n",
       " 'Association_football_midfielders',\n",
       " 'Year_of_death_missing',\n",
       " 'Main_Belt_asteroids',\n",
       " 'Asteroids_named_for_people',\n",
       " 'Year_of_birth_missing']"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(block_ranking_df['Category name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the names of the articles from the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_names_dict={}\n",
    "with open('wiki-topcats-page-names.txt') as file:\n",
    "    for line in file:\n",
    "        id_name=line.rstrip().split(' ')\n",
    "        article_names_dict[int(id_name[0])]=\" \".join(id_name[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chiasmal syndrome'"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_names_dict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of ranked categories\n",
    "cat_lst=list(block_ranking_df['Category name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[STEP1] Compute subgraph induced by input category-C0. For each node compute the sum of the weigths of the in-edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_category=cat_lst[0]\n",
    "\n",
    "sub_graph_Cat=Gtup_directed.subgraph((list(categories_edited[input_category])))#make a subgraph containing just THAT category\n",
    "in_edge_All=[] #list of first 3 sorted nodes for each category \n",
    "dict_previous_cat={} #dictionary containing a previously ranked category of the current category,key=node, value=number of in-edges \n",
    "\n",
    "#just for first category-input category C0\n",
    "in_edge_temp_dict = defaultdict(int) #temporary dictionary to store as key=node, value=number of in-edges \n",
    "for tupla in list(sub_graph_Cat.edges()): # tupla-edge tuple, (source node, destination node)\n",
    "    if tupla[1] in categories_dict[input_category]: # if the destination node belongs to the current C0 category\n",
    "        in_edge_temp_dict[tupla[1]] += 1 #add one to that destination node in order to count all the in-edges\n",
    "\n",
    "sorted_in_edge_dict= sorted(in_edge_temp_dict.items(), key=operator.itemgetter(1), reverse= True) # sort the nodes in that category\n",
    "in_edge_All.append(sorted_in_edge_dict[0:3]) #add sorted dictionary of nodes to a list \n",
    "#first2pairs = {k: mydict[k] for k in list(mydict)[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(1181827, 6), (1061932, 2), (1061429, 2)]]"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#one edge therefore one article\n",
    "in_edge_All"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[STEP2] Extend the graph to the nodes that belong to C1. Thus, for each article in C1 compute the score as before. Note that the in-edges coming from the previous category, C0, have as weights the score of the node that sends the edge.\n",
    "\n",
    "[STEP3] Repeat Step2 up to the last category of the ranking. In the last step of the example you clearly see the weight update of the edge coming from node E."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "print(len(list(sub_graph_Cat.edges())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "a={}\n",
    "a[0]=1\n",
    "a[1]=1\n",
    "b={}\n",
    "b[0]=2\n",
    "b[2]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 2, 1: 1, 2: 1}"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.update(b)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cat_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 28/28 [00:12<00:00,  6.68it/s]\n"
     ]
    }
   ],
   "source": [
    "in_edge_dict=in_edge_temp_dict #take nodes and values from the 1st input category\n",
    "#save previous category to a dict_previous_cat dictionary as a temporary variable\n",
    "for cat in tqdm(cat_lst[1:]):#for other 28 categories\n",
    "    in_edge_temp_dict = defaultdict(int)\n",
    "    sub_graph_Cat=Gtup_directed.subgraph((list(categories_edited[cat])))#make a subgraph containing just THAT category\n",
    "    for tupla in list(sub_graph_Cat.edges()): # go to every node of the cat subgraph\n",
    "        try:\n",
    "            source=in_edge_dict[tupla[0]] #take the source node from the previous category\n",
    "            in_edge_temp_dict[tupla[1]] += source #add in-edge value from the previous node\n",
    "        except:\n",
    "            in_edge_temp_dict[tupla[1]] += 1 #append 1 if it is not there\n",
    "    in_edge_dict.update(in_edge_temp_dict)\n",
    "\n",
    "    sorted_in_edge_dict= sorted(in_edge_temp_dict.items(), key=operator.itemgetter(1), reverse= True)\n",
    "    in_edge_All.append(sorted_in_edge_dict[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(in_edge_All)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(1181827, 6), (1061932, 2), (1061429, 2)],\n",
       " [(1056770, 0), (1056769, 0), (1041937, 0)],\n",
       " [(1062743, 0), (1064184, 0), (1064265, 0)],\n",
       " [(1245946, 0), (688149, 0), (688300, 0)],\n",
       " [(1061913, 3), (1061855, 2), (1061770, 2)],\n",
       " [(589847, 0), (590961, 0), (1063431, 0)],\n",
       " [(582698, 0), (590800, 0), (589828, 0)],\n",
       " [(1228798, 0), (1228799, 0), (145526, 0)],\n",
       " [(1224802, 2), (1061452, 2), (1224704, 2)],\n",
       " [(566701, 0), (328881, 0), (155018, 0)],\n",
       " [(1061902, 12), (1061932, 12), (1061913, 11)],\n",
       " [(1400478, 2), (1400534, 2), (245119, 2)],\n",
       " [(386079, 0), (384805, 0), (387201, 0)],\n",
       " [(536243, 0), (543166, 0), (543175, 0)],\n",
       " [(1365478, 2), (1365479, 2), (1365477, 2)],\n",
       " [(1400484, 4), (1400483, 4), (1061812, 4)],\n",
       " [(1734405, 0)],\n",
       " [(80379, 0), (87391, 0), (78944, 0)],\n",
       " [(81787, 0), (81941, 0), (82484, 0)],\n",
       " [(643206, 0), (973046, 0), (655275, 0)],\n",
       " [(785340, 0), (786385, 0), (786433, 0)],\n",
       " [(1358244, 0), (88262, 0), (884966, 0)],\n",
       " [(79221, 0), (81524, 0), (81919, 0)],\n",
       " [(737293, 0), (81952, 0), (81104, 0)],\n",
       " [(78944, 0), (88754, 0), (82393, 0)],\n",
       " [(1048577, 0), (1048576, 0), (1048578, 0)],\n",
       " [(870589, 0), (870768, 0), (1249929, 0)],\n",
       " [(874496, 0), (871119, 0), (871248, 0)],\n",
       " [(1048552, 0), (1048577, 0), (1048582, 0)]]"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_edge_All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=[*in_edge_All]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1181827, 6), (1061932, 2), (1061429, 2)]\n",
      "[(1056770, 0), (1056769, 0), (1041937, 0)]\n",
      "[(1062743, 0), (1064184, 0), (1064265, 0)]\n",
      "[(1245946, 0), (688149, 0), (688300, 0)]\n",
      "[(1061913, 3), (1061855, 2), (1061770, 2)]\n",
      "[(589847, 0), (590961, 0), (1063431, 0)]\n",
      "[(582698, 0), (590800, 0), (589828, 0)]\n",
      "[(1228798, 0), (1228799, 0), (145526, 0)]\n",
      "[(1224802, 2), (1061452, 2), (1224704, 2)]\n",
      "[(566701, 0), (328881, 0), (155018, 0)]\n",
      "[(1061902, 12), (1061932, 12), (1061913, 11)]\n",
      "[(1400478, 2), (1400534, 2), (245119, 2)]\n",
      "[(386079, 0), (384805, 0), (387201, 0)]\n",
      "[(536243, 0), (543166, 0), (543175, 0)]\n",
      "[(1365478, 2), (1365479, 2), (1365477, 2)]\n",
      "[(1400484, 4), (1400483, 4), (1061812, 4)]\n",
      "[(1734405, 0)]\n",
      "[(80379, 0), (87391, 0), (78944, 0)]\n",
      "[(81787, 0), (81941, 0), (82484, 0)]\n",
      "[(643206, 0), (973046, 0), (655275, 0)]\n",
      "[(785340, 0), (786385, 0), (786433, 0)]\n",
      "[(1358244, 0), (88262, 0), (884966, 0)]\n",
      "[(79221, 0), (81524, 0), (81919, 0)]\n",
      "[(737293, 0), (81952, 0), (81104, 0)]\n",
      "[(78944, 0), (88754, 0), (82393, 0)]\n",
      "[(1048577, 0), (1048576, 0), (1048578, 0)]\n",
      "[(870589, 0), (870768, 0), (1249929, 0)]\n",
      "[(874496, 0), (871119, 0), (871248, 0)]\n",
      "[(1048552, 0), (1048577, 0), (1048582, 0)]\n"
     ]
    }
   ],
   "source": [
    "for idx,row in enumerate(in_edge_All):\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 American_film_actors [('Elvis Presley', 6), ('Jennifer Jason Leigh', 2), ('Lee Marvin', 2)]\n",
      "1 British_films [('The Golden Voyage of Sinbad', 0), ('Sinbad and the Eye of the Tiger', 0), ('Cinema of the United Kingdom', 0)]\n",
      "2 English-language_films [('Reservoir Dogs', 0), ('Predators (film)', 0), ('Machete (film)', 0)]\n",
      "3 American_films [('Educating Peter', 0), ('Behind the Green Door', 0), ('Deep Throat (film)', 0)]\n",
      "4 American_television_actors [('Sean Penn', 3), ('Forest Whitaker', 2), ('Ron Howard', 2)]\n",
      "5 Black-and-white_films [('Ladki', 0), ('Penn (film)', 0), ('Duck Soup (1933 film)', 0)]\n",
      "6 Indian_films [('Shola Aur Shabnam (1992 film)', 0), ('Illarikam', 0), ('Aasha (1957 film)', 0)]\n",
      "7 English-language_albums [('Bloodrock U.S.A.', 0), ('Bloodrock 2', 0), ('Wild in the Streets (Circle Jerks album)', 0)]\n",
      "8 People_from_New_York_City [(\"Eugene O'Neill\", 2), ('Lauren Bacall', 2), ('Arthur Miller', 2)]\n",
      "9 Debut_albums [('America (America album)', 0), ('No One Can Do It Better', 0), ('The Scream (album)', 0)]\n",
      "10 Living_people [('Woody Allen', 12), ('Jennifer Jason Leigh', 12), ('Sean Penn', 11)]\n",
      "11 Harvard_University_alumni [('John F. Kennedy', 2), ('Al Gore', 2), ('Henry David Thoreau', 2)]\n",
      "12 Major_League_Baseball_pitchers [('Sparky Lyle', 0), ('Curt Simmons', 0), ('Joe Niekro', 0)]\n",
      "13 Members_of_the_United_Kingdom_Parliament_for_English_constituencies [('William Stephen Poyntz', 0), ('James Grimston, 2nd Earl of Verulam', 0), ('Edward Grimston', 0)]\n",
      "14 Place_of_birth_missing_(living_people) [('Isaac Rademacher', 2), ('Joseph Rademacher', 2), ('Jake Rademacher', 2)]\n",
      "15 American_military_personnel_of_World_War_II [('Richard Nixon', 4), ('Jimmy Carter', 4), ('Paul Newman', 4)]\n",
      "16 Windows_games [('The 7th Guest', 0)]\n",
      "17 English_footballers [('Stuart Ripley', 0), ('Jason Steele (footballer)', 0), ('Kyle Bennett (footballer)', 0)]\n",
      "18 The_Football_League_players [('Mick McCarthy', 0), ('Glenn Hoddle', 0), ('Ian Harte', 0)]\n",
      "19 Year_of_birth_missing_(living_people) [('Andy Wellings', 0), ('Jean Donnelly', 0), ('Laci Scott', 0)]\n",
      "20 Rivers_of_Romania [('Lona River', 0), ('Cerna River (Mure)', 0), ('Sterminos River (Cerna)', 0)]\n",
      "21 Association_football_forwards [('Christian Stuani', 0), ('Alessandro Altobelli', 0), ('Fernando Morena', 0)]\n",
      "22 Association_football_defenders [('Aaron Lescott', 0), ('Sylvain Distin', 0), ('Phil Jagielka', 0)]\n",
      "23 Association_football_goalkeepers [('Jos Pereira', 0), ('Goalkeeper (association football)', 0), ('Maik Taylor', 0)]\n",
      "24 Association_football_midfielders [('Kyle Bennett (footballer)', 0), ('Simone Perrotta', 0), ('David Beckham', 0)]\n",
      "25 Year_of_death_missing [('Richard Witton', 0), ('Thomas Benwell', 0), ('Robert Burton (academic)', 0)]\n",
      "26 Main_Belt_asteroids [('Asteroid belt', 0), ('19367 Pink Floyd', 0), ('3237 Victorplatt', 0)]\n",
      "27 Asteroids_named_for_people [('6075 Zajtsev', 0), ('153 Hilda', 0), ('3749 Balam', 0)]\n",
      "28 Year_of_birth_missing [('Thomas Caius', 0), ('Richard Witton', 0), ('John Martyn (academic)', 0)]\n"
     ]
    }
   ],
   "source": [
    "for idx,row in enumerate(in_edge_All):\n",
    "    row_lst=[]\n",
    "    for tup in row:\n",
    "        tuple_new=(article_names_dict[tup[0]],tup[1])\n",
    "        row_lst.append(tuple_new)\n",
    "    print(idx,cat_lst[idx],row_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
